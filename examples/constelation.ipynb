{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7ffac9-f13d-46ca-938a-cd9bbb40c38d",
   "metadata": {},
   "source": [
    "# Setting Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14ecaa-6f76-4101-852e-7b2bf7fed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR) # we can worry about little warnings later this is just an example of the overarching I dont want to go too deep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d24b05-d618-45d3-8f53-ddeb1a751ff9",
   "metadata": {},
   "source": [
    "# Get Constelation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c582b93-0024-4922-95d0-46f1159bdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "def get_file(url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"Skipping download {} already exists.\".format(path))\n",
    "        return None\n",
    "    print(\"Downloading {} to {} ...\".format(url, path))\n",
    "    directory, file = os.path.split(os.path.abspath(path))\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    r = requests.get(url, allow_redirects=True, verify=False)\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Complete\")\n",
    "        \n",
    "def unzip(path, dest):\n",
    "    if os.path.exists(dest):\n",
    "        print(\"Skipping unzip as {} already exists.\".format(dest))\n",
    "        return None\n",
    "    os.mkdir(dest)\n",
    "    with zipfile.ZipFile(path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af348b14-6633-4815-a47b-bd1622e22a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate this with url to your data\n",
    "data_url = \"****\"\n",
    "data_path = os.path.join(os.getcwd(), \"datasets/constelation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dff11-7859-48a1-94cd-17ff58c05290",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file(data_url, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf29f67-93f0-475a-94be-ccf7a849e69b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wrangle Dataset\n",
    "\n",
    "This will be quite specific to our data so you may need to modify this to your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92413c-374d-458f-8bb2-9ac80882d2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(data_path).drop(\"totyield\", axis=1).sort_values(by=[\"milk_date\"])\n",
    "data[\"milk_date\"] = pd.to_datetime(data[\"milk_date\"], format='%Y-%m-%d')\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613114d-86be-4344-9e4e-b826219dee7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "inputs = {\"x\":[], \n",
    "          \"y\":[], \n",
    "#           \"context\":[]\n",
    "         }\n",
    "window_size = 21\n",
    "\n",
    "for group in data.groupby([\"itb\"]):\n",
    "    # brief calculation of days since last milking\n",
    "    group = group[1] # get rid of tuple abstraction by groupby\n",
    "    group[\"previous_milk_date\"] =  group[\"milk_date\"].shift(1)\n",
    "    group[\"days_unmilked\"] = (group[\"milk_date\"] - group[\"previous_milk_date\"]).dt.days / 30 # quic/ rough normalisation assuming max 30 days\n",
    "    group.dropna(inplace=True)\n",
    "    # dropping all non numeric types that we arent interested in\n",
    "    example = group.select_dtypes(include=[np.number])\n",
    "#     if i == 0:\n",
    "#         print(\"grouped df\", group)\n",
    "#         print(\"numeric df\", example)\n",
    "    # caputre a rolling window going from oldest to newest\n",
    "    # so the network cant be biased since it wont have seen the outcome before\n",
    "    for window in example.rolling(window=window_size):\n",
    "        if len(window[\"milkyield\"]) == window_size:\n",
    "#             if j == 0:\n",
    "#                 print(\"Cleaned df:\", window.iloc[:-1, :])\n",
    "            # need to tell the network what goes where/ to what node\n",
    "            inputs[\"x\"].append(window.iloc[:-1, :].to_numpy()) # historic data in general except last (truth that it shouldnt see)\n",
    "            inputs[\"y\"].append(np.array(window[\"milkyield\"].iloc[-1])) # ground truth value we expect I.E last one since in ascending cronology as we descend\n",
    "#             inputs[\"context\"].append(np.array(window[\"days_unmilked\"].iloc[-1])) # get some additional context relevant to prediction\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "print(len(inputs[\"x\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da66107-7094-4a61-ac31-b7032f4e5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(inputs, train_fraction=0.7):\n",
    "    \"\"\"Create a test train split from one of our standard input dictionaries splitting it in two (non-montecarlo).\n",
    "    \n",
    "    We dont want to split using montecarlo as the order is in ascending cronology so we want to split through time so the testing set contains data that is ahead in time to the training set.\n",
    "    \"\"\"\n",
    "    nodes = list(inputs.keys())\n",
    "    train = {}\n",
    "    test = {}\n",
    "    train_count = int(np.round(len(inputs[nodes[0]]) * train_fraction, decimals=0))\n",
    "    test_count = len(inputs[nodes[0]]) - train_count\n",
    "    \n",
    "    for key, values in inputs.items():\n",
    "        train[key] = values[:train_count]\n",
    "        test[key] = values[train_count:]\n",
    "        \n",
    "    print(train_count, test_count)\n",
    "    return train, test\n",
    "\n",
    "    \n",
    "train_dict, test_dict = train_test_split(inputs=inputs)\n",
    "len(train_dict[\"x\"]), len(test_dict[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927870e-23e4-4476-96fe-2ccc54cb65ec",
   "metadata": {},
   "source": [
    "# Get Constelation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab39c49-0830-492b-82f7-584a16b5646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhez.nn.graph.prefab import milky, cnn_regressor\n",
    "\n",
    "# network = milky(data_shape=inputs[\"x\"][0].shape, filter_length=5, stride=3)\n",
    "network = cnn_regressor(data_shape=inputs[\"x\"][0].shape, filter_length=5, stride=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c4cf9-4f89-453a-9585-5a5ca5a5db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def strip(graph):\n",
    "    g = copy.deepcopy(graph)\n",
    "    for node in g.nodes(data=True):\n",
    "        try:\n",
    "            # node[1][\"title\"] = \"{}:\\n{}\".format(type(node[1][\"node\"]), repr(node[1][\"node\"]))\n",
    "            del node[1][\"node\"]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd5759-7448-4db4-bbd6-1ce9c66dcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "stripped = strip(network)\n",
    "print(stripped)\n",
    "\n",
    "from pyvis.network import Network\n",
    "net = Network('700px', '700px', bgcolor='#222222', font_color='white', notebook=True)\n",
    "net.from_nx(stripped)\n",
    "# net.show_buttons(filter_=\"physics\")\n",
    "net.show(\"constelation.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f4805-f1c4-4397-b105-7d5b714ec7c7",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13ee12-c63b-47d4-84f6-04fb11dedf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhez.nn.graph.utils import train, infer\n",
    "from fhez.nn.loss.mse import MSE\n",
    "output = train(graph=network, inputs=train_dict, batch_size=3, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a4e931-7c73-4d28-8bb4-8af7978fbd7a",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d38a5e-a479-4d03-afe3-f7ce534d53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this awkward looking syntax is just optimising by keeping the same dict and passing in a subset of keys them immediateley dropping results we dont care for\n",
    "# you could also just run things without these minor optimisations like: results=infer(network, inputs) then select the results you care for so in our case those on the y_hat node as infer will return a dictionary of all terminal node outputs\n",
    "y_hats = infer(graph=network, inputs={key:value for key,value in train_dict.items() if key in [\"x\"]})[\"y_hat\"]\n",
    "training_loss = MSE().forward(y=train_dict[\"y\"], y_hat=np.array(y_hats))\n",
    "training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a03aa-2f08-4754-895b-2b2ad2eae0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhez.nn.loss.mae import MAE\n",
    "mae_training = MAE().forward(y=train_dict[\"y\"], y_hat=np.array(y_hats))\n",
    "mae_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72742ea6-517e-469a-80b7-d68dc12c4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loss\n",
    "y_hats = infer(graph=network, inputs={key:value for key,value in test_dict.items() if key in [\"x\"]})[\"y_hat\"] \n",
    "testing_loss = MSE().forward(y=test_dict[\"y\"], y_hat=np.array(y_hats))\n",
    "testing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef6bf2-7864-483e-9ec5-a0c85cb93e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_testing = MAE().forward(y=test_dict[\"y\"], y_hat=np.array(y_hats))\n",
    "mae_testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
